import os
import json
import time
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from pathlib import Path
import requests
from dotenv import load_dotenv
from shapely.geometry import shape, Point

# =========================
# 0) –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –∫–ª—é—á
# =========================
load_dotenv(dotenv_path="data/.env")
DGIS_KEY = os.getenv("DGIS_KEY", "a07319d9-8b64-45fd-bd13-5424148ad318")
if not DGIS_KEY:
    raise RuntimeError("–ù–µ—Ç –∫–ª—é—á–∞ DGIS_KEY –≤ data/.env")
print("DGIS_KEY:", DGIS_KEY[:8] + "...")

CATALOG_API = "https://catalog.api.2gis.com/3.0/items"
POLYGON_PATH = os.getenv("POLYGON_PATH", "data/kirovsky.geojson")
NX = int(os.getenv("NX", "3"))  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –ø–æ–∫—Ä—ã—Ç–∏—è –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
NY = int(os.getenv("NY", "3"))

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
MASTER_PARQUET = "data/kirovsky_all.parquet"

# =========================
# 1) –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ Parquet —Ñ–∞–π–ª–æ–º
# =========================
def load_existing_ids() -> set:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ ID –∏–∑ –º–∞—Å—Ç–µ—Ä-—Ñ–∞–π–ª–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏."""
    if not os.path.exists(MASTER_PARQUET):
        return set()
    
    try:
        table = pq.read_table(MASTER_PARQUET, columns=['id'])
        return set(table.column('id').to_pylist())
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {MASTER_PARQUET}: {e}")
        return set()

def append_to_parquet(new_items: list, existing_ids: set) -> set:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ items –≤ Parquet —Ñ–∞–π–ª, –∏–∑–±–µ–≥–∞—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä ID.
    """
    if not new_items:
        return existing_ids
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    unique_new_items = []
    for item in new_items:
        item_id = item.get('id')
        if item_id and item_id not in existing_ids:
            unique_new_items.append(item)
            existing_ids.add(item_id)
    
    if not unique_new_items:
        print("ü§∑ –í—Å–µ –æ–±—ä–µ–∫—Ç—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        return existing_ids
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame(unique_new_items)
    
    # –î–æ–±–∞–≤–ª—è–µ–º timestamp —Å–±–æ—Ä–∞
    df['_collected_at'] = pd.Timestamp.now()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    if os.path.exists(MASTER_PARQUET):
        # Append –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É —Ñ–∞–π–ª—É
        existing_table = pq.read_table(MASTER_PARQUET)
        new_table = pa.Table.from_pandas(df, schema=existing_table.schema)
        combined_table = pa.concat_tables([existing_table, new_table])
        pq.write_table(combined_table, MASTER_PARQUET)
    else:
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª
        os.makedirs(os.path.dirname(MASTER_PARQUET), exist_ok=True)
        pq.write_table(pa.Table.from_pandas(df), MASTER_PARQUET)
    
    print(f"üíæ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(unique_new_items)} –∑–∞–ø–∏—Å–µ–π –≤ {MASTER_PARQUET}")
    return existing_ids

# =========================
# 2) –ì–µ–æ-—É—Ç–∏–ª–∏—Ç—ã
# =========================
def geojson_geom(geojson_path: str):
    """–ß–∏—Ç–∞–µ—Ç Polygon/MultiPolygon –∏–∑ GeoJSON."""
    with open(geojson_path, "r", encoding="utf-8") as f:
        gj = json.load(f)
    if gj.get("type") == "FeatureCollection":
        geom = shape(gj["features"][0]["geometry"])
    elif gj.get("type") == "Feature":
        geom = shape(gj["geometry"])
    else:
        geom = shape(gj)
    return geom

def make_tiles(minx: float, miny: float, maxx: float, maxy: float, nx: int, ny: int):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç bbox –Ω–∞ nx√óny —Ç–∞–π–ª–æ–≤."""
    tiles = []
    dx = (maxx - minx) / nx
    dy = (maxy - miny) / ny
    for j in range(ny):
        lat_top = maxy - j * dy
        lat_bottom = maxy - (j + 1) * dy
        for i in range(nx):
            lon_left = minx + i * dx
            lon_right = minx + (i + 1) * dx
            point1 = f"{lon_left},{lat_top}"
            point2 = f"{lon_right},{lat_bottom}"
            tiles.append((point1, point2))
    return tiles

# =========================
# 3) –†–∞–±–æ—Ç–∞ —Å API
# =========================
def fetch_page(params: dict, page: int = 1, page_size: int = 10) -> dict:
    """
    –ó–∞–ø—Ä–æ—Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã; –ª–∏–º–∏—Ç—ã 2–ì–ò–°: page_size 1..10, page 1..5.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å –∫–ª—é—á–∞–º–∏ items/total. 404 —Ç—Ä–∞–∫—Ç—É–µ–º –∫–∞–∫ –ø—É—Å—Ç—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É.
    """
    if not (1 <= page_size <= 10):
        raise ValueError("page_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 1..10")
    if not (1 <= page <= 5):
        raise ValueError("page –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 1..5")

    p = dict(params)
    p.update({"page": page, "page_size": page_size, "key": DGIS_KEY})
    
    try:
        r = requests.get(CATALOG_API, params=p, timeout=30)
    except requests.exceptions.Timeout:
        print(f"       ‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}")
        return {"items": [], "total": 0}
    except requests.exceptions.ConnectionError:
        print(f"       üîå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}")
        return {"items": [], "total": 0}
    except Exception as e:
        print(f"       ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        return {"items": [], "total": 0}

    # HTTP 404 ‚Äî –ø—É—Å—Ç–æ
    if r.status_code == 404:
        return {"items": [], "total": 0}
    if r.status_code != 200:
        print(f"       ‚ùå HTTP {r.status_code} –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}")
        return {"items": [], "total": 0}

    data = r.json()
    meta = data.get("meta", {})
    code = meta.get("code")

    # –ö–æ–¥ 404 –≤ meta ‚Äî —Ç–æ–∂–µ –ø—É—Å—Ç–æ
    if code == 404:
        return {"items": [], "total": 0}

    if code != 200:
        error_msg = meta.get('error', {}).get('message') or 'Unknown error'
        print(f"       ‚ùå 2GIS error {code}: {error_msg}")
        return {"items": [], "total": 0}

    result = data.get("result") or {}
    result.setdefault("items", [])
    result.setdefault("total", 0)
    return result

def fetch_all(params: dict, sleep: float = 0.3, page_size: int = 10) -> list:
    """–ü–µ—Ä–µ–±–æ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü –≤ —Ä–∞–º–∫–∞—Ö –ª–∏–º–∏—Ç–∞ API: page 1..5. –ü—É—Å—Ç—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (404) –ø—Ä–æ–ø—É—Å–∫–∞–µ–º."""
    items_all = []
    for page in range(1, 6):
        res = fetch_page(params, page=page, page_size=page_size)
        items = res.get("items", []) or []
        total = res.get("total", 0) or 0

        if not items:
            break

        items_all.extend(items)
        print(f"       üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {len(items)} –æ–±—ä–µ–∫—Ç–æ–≤")

        if total and page * page_size >= total:
            break

        time.sleep(sleep)

    return items_all

# =========================
# 4) –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
# =========================
def save_category_files(items: list, out_prefix: str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ CSV –∏ GeoJSON."""
    
    if not items:
        print("     üì≠ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return
    
    csv_path = f"data/{out_prefix}.csv"
    gj_path = f"data/{out_prefix}.geojson"
    
    # CSV
    os.makedirs(os.path.dirname(csv_path), exist_ok=True)
    csv_data = []
    for it in items:
        pt = it.get("point") or {}
        rubrics = ",".join([r.get("name", "") for r in it.get("rubrics", [])])
        csv_data.append({
            "id": it.get("id"),
            "name": it.get("name"),
            "address_name": it.get("address_name") or (it.get("address") or {}).get("name"),
            "lon": pt.get("lon"),
            "lat": pt.get("lat"),
            "rubrics": rubrics,
        })
    
    df = pd.DataFrame(csv_data)
    df.to_csv(csv_path, index=False, encoding='utf-8')
    
    # GeoJSON
    feats = []
    for it in items:
        pt = it.get("point")
        if not pt:
            continue
        feats.append({
            "type": "Feature",
            "properties": {
                "id": it.get("id"),
                "name": it.get("name"),
                "address_name": it.get("address_name") or (it.get("address") or {}).get("name"),
                "rubrics": ",".join([r.get("name", "") for r in it.get("rubrics", [])]),
            },
            "geometry": {"type": "Point", "coordinates": [pt["lon"], pt["lat"]]},
        })
    
    fc = {"type": "FeatureCollection", "features": feats}
    with open(gj_path, "w", encoding="utf-8") as f:
        json.dump(fc, f, ensure_ascii=False, indent=2)
    
    print(f"     üíæ –§–∞–π–ª—ã: {os.path.basename(csv_path)}, {os.path.basename(gj_path)}")

# =========================
# 5) –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å: —Ç–∞–π–ª–∏–Ω–≥ + –¥–µ–¥—É–ø + —Ñ–∏–ª—å—Ç—Ä –ø–æ –ø–æ–ª–∏–≥–æ–Ω—É
# =========================
def pull_for_polygon(geojson_path: str, query_text: str, out_prefix: str, 
                    existing_ids: set, nx: int = NX, ny: int = NY) -> set:
    """
    –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ª–∏–≥–æ–Ω–∞ —Å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –≤ Parquet.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä existing_ids.
    """
    poly = geojson_geom(geojson_path)
    minx, miny, maxx, maxy = poly.bounds

    tiles = make_tiles(minx, miny, maxx, maxy, nx=nx, ny=ny)
    print(f"   üó∫Ô∏è  –¢–∞–π–ª–æ–≤: {len(tiles)} ({nx}√ó{ny})")

    # —Å–æ–±–∏—Ä–∞–µ–º —Å–æ –≤—Å–µ—Ö —Ç–∞–π–ª–æ–≤, –¥–µ–¥—É–ø –ø–æ id
    all_items = []
    successful_tiles = 0
    
    for idx, (point1, point2) in enumerate(tiles, start=1):
        params = {
            "q": query_text,
            "type": "branch",
            "point1": point1,   # –ª–µ–≤—ã–π-–≤–µ—Ä—Ö–Ω–∏–π
            "point2": point2,   # –ø—Ä–∞–≤—ã–π-–Ω–∏–∂–Ω–∏–π
            "fields": "items.point,items.address,items.rubrics",
        }
        print(f"     üìç –¢–∞–π–ª {idx}/{len(tiles)}")
        batch = fetch_all(params, page_size=10)
        
        if batch:
            successful_tiles += 1
            print(f"       ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(batch)} –æ–±—ä–µ–∫—Ç–æ–≤")
        else:
            print(f"       ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤")
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ–ª–∏–≥–æ–Ω—É
        filtered_batch = []
        for it in batch:
            p = it.get("point")
            if p and poly.contains(Point(p["lon"], p["lat"])):
                filtered_batch.append(it)
        
        all_items.extend(filtered_batch)
        time.sleep(0.2)  # —É–º–µ–Ω—å—à–∏–ª–∏ –ø–∞—É–∑—É –º–µ–∂–¥—É —Ç–∞–π–ª–∞–º–∏

    print(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —Ç–∞–π–ª–æ–≤: {successful_tiles}/{len(tiles)}")

    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    unique_items = []
    seen_ids = set()
    for item in all_items:
        item_id = item.get('id')
        if item_id and item_id not in seen_ids:
            unique_items.append(item)
            seen_ids.add(item_id)
    
    print(f"   üîç –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {len(unique_items)}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    save_category_files(unique_items, out_prefix)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—Å–Ω–æ–≤–Ω–æ–π Parquet
    updated_ids = append_to_parquet(unique_items, existing_ids)
    
    return updated_ids

# =========================
# 6) –ó–∞–ø—É—Å–∫
# =========================
def get_parquet_stats():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–º—É Parquet —Ñ–∞–π–ª—É."""
    if not os.path.exists(MASTER_PARQUET):
        return 0, 0.0
    
    try:
        table = pq.read_table(MASTER_PARQUET)
        count = table.num_rows
        size_mb = os.path.getsize(MASTER_PARQUET) / (1024 * 1024)
        return count, size_mb
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        return 0, 0.0

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–æ–ª–∏–≥–æ–Ω–∞
    if not os.path.exists(POLYGON_PATH):
        print(f"‚ùå –§–∞–π–ª –ø–æ–ª–∏–≥–æ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {POLYGON_PATH}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª kirovsky.geojson –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø–∞–ø–∫–µ data/")
        exit(1)
    
    print("=" * 60)
    print("üó∫Ô∏è  2–ì–ò–° –°–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ö–∏—Ä–æ–≤—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ ID –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
    existing_ids = load_existing_ids()
    initial_count, initial_size = get_parquet_stats()
    print(f"üìä –ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤: {initial_count}")
    print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä Parquet: {initial_size:.2f} MB")
    
    # –°–û–ö–†–ê–©–ï–ù–ù–´–ô —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π (15 –≤–º–µ—Å—Ç–æ 50+)
    rubrics = [
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –µ–¥—ã
        ("–∫–∞—Ñ–µ", "food"),
        ("—Ä–µ—Å—Ç–æ—Ä–∞–Ω", "food"),
        ("—Å—Ç–æ–ª–æ–≤–∞—è", "food"),
        
        # –ó–¥–æ—Ä–æ–≤—å–µ –∏ –º–µ–¥–∏—Ü–∏–Ω–∞
        ("–∞–ø—Ç–µ–∫–∞", "health"),
        ("–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞", "health"),
        
        # –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
        ("—à–∫–æ–ª–∞", "education"),
        ("–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥", "education"),
        
        # –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç
        ("–æ—Å—Ç–∞–Ω–æ–≤–∫–∞", "transport"),
        ("–ø–∞—Ä–∫–æ–≤–∫–∞", "transport"),
        ("–º–µ—Ç—Ä–æ", "transport"),
        
        # –ú–∞–≥–∞–∑–∏–Ω—ã
        ("—Å—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç", "retail"),
        ("–º–∞–≥–∞–∑–∏–Ω", "retail"),
        
        # –£—Å–ª—É–≥–∏
        ("–±–∞–Ω–∫", "services"),
        ("–±–∞–Ω–∫–æ–º–∞—Ç", "services"),
        
        # –†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è
        ("–∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä", "entertainment"),
    ]

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä—É–±—Ä–∏–∫–∏
    total_categories = len(rubrics)
    print(f"\nüîç –ë—É–¥–µ—Ç —Å–æ–±—Ä–∞–Ω–æ {total_categories} –∫–ª—é—á–µ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
    print("‚è∞ –û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–æ–µ –≤—Ä–µ–º—è: 20-40 –º–∏–Ω—É—Ç")
    
    start_time = time.time()
    
    for i, (query_text, cls) in enumerate(rubrics, 1):
        safe_name = f"kirovsky_{cls}_{query_text.replace(' ', '_')}"
        print(f"\n[{i}/{total_categories}] üîç –°–±–æ—Ä: '{query_text}'")
        print("-" * 40)
        
        try:
            existing_ids = pull_for_polygon(POLYGON_PATH, query_text, safe_name, existing_ids, nx=NX, ny=NY)
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{query_text}': {e}")
            print("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–æ —Å–ª–µ–¥—É—é—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π...")
            continue
        
        # –£–º–µ–Ω—å—à–µ–Ω–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
        if i < total_categories:
            print("     ‚è≥ –ü–∞—É–∑–∞ 1 —Å–µ–∫—É–Ω–¥–∞...")
            time.sleep(1)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    end_time = time.time()
    elapsed_time = end_time - start_time
    final_count, final_size = get_parquet_stats()
    
    print(f"\n{'='*60}")
    print(f"üéâ –ë–´–°–¢–†–´–ô –°–ë–û–† –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù!")
    print(f"{'='*60}")
    print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤: {final_count}")
    print(f"   ‚Ä¢ –î–æ–±–∞–≤–ª–µ–Ω–æ –∑–∞ —Å–µ—Å—Å–∏—é: {final_count - initial_count}")
    print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä Parquet —Ñ–∞–π–ª–∞: {final_size:.2f} MB")
    print(f"   ‚Ä¢ –ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è: {elapsed_time/60:.1f} –º–∏–Ω—É—Ç")
    print(f"   ‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏–π —Å–æ–±—Ä–∞–Ω–æ: {total_categories}")
    print(f"\nüíæ –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª: {MASTER_PARQUET}")
    
    # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ–∑–∂–µ
    if final_count > initial_count:
        print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω—ã –∫–ª—é—á–µ–≤—ã–µ POI!")
        print("   –ß—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –ø—Ä–æ—Å—Ç–æ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç —Å–Ω–æ–≤–∞")
        print("   —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º —Å–ø–∏—Å–∫–æ–º rubrics")